{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "import math  \n",
    "import numpy as np  \n",
    "import tensorflow as tf  \n",
    "import matplotlib.pyplot as plt  \n",
    "  \n",
    "#============================================================================  \n",
    "#-----------------生成图片路径和标签的List------------------------------------  \n",
    "  \n",
    "train_dir = 'C:/Users/sungroup/PycharmProjects/anti_cnn/pavement'  \n",
    "  \n",
    "bhg = []  \n",
    "label_bhg = []  \n",
    "hg = []  \n",
    "label_hg = []  \n",
    "  \n",
    "#step1：获取'G:/git_cache/anti_cnn/pavement'下所有的图片路径名，存放到  \n",
    "#对应的列表中，同时贴上标签，存放到label列表中。  \n",
    "def get_files(file_dir, ratio):  \n",
    "    for file in os.listdir(file_dir+'/bhg'):  \n",
    "        bhg.append(file_dir +'/bhg'+'/'+ file)   \n",
    "        label_bhg.append(0)  \n",
    "    for file in os.listdir(file_dir+'/hg'):  \n",
    "        hg.append(file_dir +'/hg'+'/'+file)  \n",
    "        label_hg.append(1)  \n",
    "  \n",
    "#step2：对生成的图片路径和标签List做打乱处理把bhg和hg合起来组成一个list（img和lab）  \n",
    "    image_list = np.hstack((bhg, hg))  \n",
    "    label_list = np.hstack((label_bhg, label_hg))  \n",
    "  \n",
    "    #利用shuffle打乱顺序  \n",
    "    temp = np.array([image_list, label_list])  \n",
    "    temp = temp.transpose()  \n",
    "    np.random.shuffle(temp)  \n",
    "      \n",
    "    #从打乱的temp中再取出list（img和lab）  \n",
    "    #image_list = list(temp[:, 0])  \n",
    "    #label_list = list(temp[:, 1])  \n",
    "    #label_list = [int(i) for i in label_list]  \n",
    "    #return image_list, label_list  \n",
    "      \n",
    "    #将所有的img和lab转换成list  \n",
    "    all_image_list = list(temp[:, 0])  \n",
    "    all_label_list = list(temp[:, 1])  \n",
    "  \n",
    "    #将所得List分为两部分，一部分用来训练tra，一部分用来测试val  \n",
    "    #ratio是测试集的比例  \n",
    "    n_sample = len(all_label_list)  \n",
    "    n_val = int(math.ceil(n_sample*ratio))   #测试样本数  \n",
    "    n_train = n_sample - n_val   #训练样本数  \n",
    "  \n",
    "    tra_images = all_image_list[0:n_train]  \n",
    "    tra_labels = all_label_list[0:n_train]  \n",
    "    tra_labels = [int(float(i)) for i in tra_labels]  \n",
    "    val_images = all_image_list[n_train:-1]  \n",
    "    val_labels = all_label_list[n_train:-1]  \n",
    "    val_labels = [int(float(i)) for i in val_labels]  \n",
    "  \n",
    "    return tra_images, tra_labels, val_images, val_labels  \n",
    "      \n",
    "      \n",
    "#---------------------------------------------------------------------------  \n",
    "#--------------------生成Batch----------------------------------------------  \n",
    "  \n",
    "#step1：将上面生成的List传入get_batch() ，转换类型，产生一个输入队列queue，因为img和lab  \n",
    "#是分开的，所以使用tf.train.slice_input_producer()，然后用tf.read_file()从队列中读取图像  \n",
    "#   image_W, image_H, ：设置好固定的图像高度和宽度  \n",
    "#   设置batch_size：每个batch要放多少张图片  \n",
    "#   capacity：一个队列最大多少  \n",
    "def get_batch(image, label, image_W, image_H, batch_size, capacity):  \n",
    "    #转换类型  \n",
    "    image = tf.cast(image, tf.string)  \n",
    "    label = tf.cast(label, tf.int32)  \n",
    "  \n",
    "    # make an input queue  \n",
    "    input_queue = tf.train.slice_input_producer([image, label])  \n",
    "  \n",
    "    label = input_queue[1]  \n",
    "    image_contents = tf.read_file(input_queue[0]) #read img from a queue    \n",
    "      \n",
    "#step2：将图像解码，不同类型的图像不能混在一起，要么只用jpeg，要么只用png等。  \n",
    "    image = tf.image.decode_jpeg(image_contents, channels=3)   \n",
    "      \n",
    "#step3：数据预处理，对图像进行旋转、缩放、裁剪、归一化等操作，让计算出的模型更健壮。  \n",
    "    #image = tf.image.resize_image_with_crop_or_pad(image, image_W, image_H)\n",
    "    image = tf.image.resize_images(image, (image_W, image_H))\n",
    "    image = tf.image.per_image_standardization(image)  \n",
    "  \n",
    "#step4：生成batch  \n",
    "#image_batch: 4D tensor [batch_size, width, height, 3],dtype=tf.float32   \n",
    "#label_batch: 1D tensor [batch_size], dtype=tf.int32  \n",
    "    image_batch, label_batch = tf.train.batch([image, label],  \n",
    "                                                batch_size= batch_size,  \n",
    "                                                num_threads= 32,   \n",
    "                                                capacity = capacity)  \n",
    "    #重新排列label，行数为[batch_size]  \n",
    "    label_batch = tf.reshape(label_batch, [batch_size])  \n",
    "    image_batch = tf.cast(image_batch, tf.float32)  \n",
    "    return image_batch, label_batch              \n",
    "  \n",
    "#========================================================================</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=========================================================================  \n",
    "import tensorflow as tf  \n",
    "#=========================================================================  \n",
    "#网络结构定义  \n",
    "    #输入参数：images，image batch、4D tensor、tf.float32、[batch_size, width, height, channels]  \n",
    "    #返回参数：logits, float、 [batch_size, n_classes]  \n",
    "def inference(images, batch_size, n_classes):  \n",
    "#一个简单的卷积神经网络，卷积+池化层x2，全连接层x2，最后一个softmax层做分类。  \n",
    "#卷积层1  \n",
    "#64个3x3的卷积核（3通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()  \n",
    "    with tf.variable_scope('conv1') as scope:  \n",
    "          \n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[3,3,3,64], stddev = 1.0, dtype = tf.float32),   \n",
    "                              name = 'weights', dtype = tf.float32)  \n",
    "          \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [64]),  \n",
    "                             name = 'biases', dtype = tf.float32)  \n",
    "          \n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')  \n",
    "        pre_activation = tf.nn.bias_add(conv, biases)  \n",
    "        conv1 = tf.nn.relu(pre_activation, name= scope.name)  \n",
    "          \n",
    "#池化层1  \n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，局部响应归一化，对训练有利。  \n",
    "    with tf.variable_scope('pooling1_lrn') as scope:  \n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME', name='pooling1')  \n",
    "        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')  \n",
    "  \n",
    "#卷积层2  \n",
    "#16个3x3的卷积核（16通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()  \n",
    "    with tf.variable_scope('conv2') as scope:  \n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[3,3,64,16], stddev = 0.1, dtype = tf.float32),   \n",
    "                              name = 'weights', dtype = tf.float32)  \n",
    "          \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [16]),  \n",
    "                             name = 'biases', dtype = tf.float32)  \n",
    "          \n",
    "        conv = tf.nn.conv2d(norm1, weights, strides = [1,1,1,1],padding='SAME')  \n",
    "        pre_activation = tf.nn.bias_add(conv, biases)  \n",
    "        conv2 = tf.nn.relu(pre_activation, name='conv2')  \n",
    "  \n",
    "#池化层2  \n",
    "#3x3最大池化，步长strides为2，池化后执行lrn()操作，  \n",
    "    #pool2 and norm2  \n",
    "    with tf.variable_scope('pooling2_lrn') as scope:  \n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,beta=0.75,name='norm2')  \n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME',name='pooling2')  \n",
    "  \n",
    "#全连接层3  \n",
    "#128个神经元，将之前pool层的输出reshape成一行，激活函数relu()  \n",
    "    with tf.variable_scope('local3') as scope:  \n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])  \n",
    "        dim = reshape.get_shape()[1].value  \n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[dim,128], stddev = 0.005, dtype = tf.float32),  \n",
    "                             name = 'weights', dtype = tf.float32)  \n",
    "          \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]),   \n",
    "                             name = 'biases', dtype=tf.float32)  \n",
    "          \n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)  \n",
    "          \n",
    "#全连接层4  \n",
    "#128个神经元，激活函数relu()   \n",
    "    with tf.variable_scope('local4') as scope:  \n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[128,128], stddev = 0.005, dtype = tf.float32),  \n",
    "                              name = 'weights',dtype = tf.float32)  \n",
    "          \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]),  \n",
    "                             name = 'biases', dtype = tf.float32)  \n",
    "          \n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')  \n",
    "  \n",
    "#dropout层          \n",
    "    with tf.variable_scope('dropout') as scope:  \n",
    "        drop_out = tf.nn.dropout(local4, 0.8)  \n",
    "              \n",
    "          \n",
    "#Softmax回归层  \n",
    "#将前面的FC层输出，做一个线性回归，计算出每一类的得分，在这里是2类，所以这个层输出的是两个得分。  \n",
    "    with tf.variable_scope('softmax_linear') as scope:  \n",
    "        weights = tf.Variable(tf.truncated_normal(shape=[128, n_classes], stddev = 0.005, dtype = tf.float32),  \n",
    "                              name = 'softmax_linear', dtype = tf.float32)  \n",
    "          \n",
    "        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [n_classes]),  \n",
    "                             name = 'biases', dtype = tf.float32)  \n",
    "          \n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')  \n",
    "  \n",
    "    return softmax_linear  \n",
    "  \n",
    "#-----------------------------------------------------------------------------  \n",
    "#loss计算  \n",
    "    #传入参数：logits，网络计算输出值。labels，真实值，在这里是0或者1  \n",
    "    #返回参数：loss，损失值  \n",
    "def losses(logits, labels):  \n",
    "    with tf.variable_scope('loss') as scope:  \n",
    "        cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='xentropy_per_example')  \n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')  \n",
    "        tf.summary.scalar(scope.name+'/loss', loss)  \n",
    "    return loss  \n",
    "  \n",
    "#--------------------------------------------------------------------------  \n",
    "#loss损失值优化  \n",
    "    #输入参数：loss。learning_rate，学习速率。  \n",
    "    #返回参数：train_op，训练op，这个参数要输入sess.run中让模型去训练。  \n",
    "def trainning(loss, learning_rate):  \n",
    "    with tf.name_scope('optimizer'):  \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)  \n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)  \n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)  \n",
    "    return train_op  \n",
    "  \n",
    "#-----------------------------------------------------------------------  \n",
    "#评价/准确率计算  \n",
    "    #输入参数：logits，网络计算值。labels，标签，也就是真实值，在这里是0或者1。  \n",
    "    #返回参数：accuracy，当前step的平均准确率，也就是在这些batch中多少张图片被正确分类了。  \n",
    "def evaluation(logits, labels):  \n",
    "    with tf.variable_scope('accuracy') as scope:  \n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)  \n",
    "        correct = tf.cast(correct, tf.float16)  \n",
    "        accuracy = tf.reduce_mean(correct)  \n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)  \n",
    "    return accuracy  \n",
    "  \n",
    "#========================================================================</span>  \n",
    "def print_confusion_matrix(logits, labels):\n",
    "    with tf.variable_scope('cm') as scope:  \n",
    "        cls_true = np.array(labels)\n",
    "        cls_pred = np.array(logits)\n",
    "        cm = confusion_matrix(y_true=cls_true,\n",
    "                              y_pred=cls_pred)  \n",
    "        tf.summary.scalar(scope.name+'/cm', cm)  \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, train loss = 0.69, train accuracy = 38.50%\n",
      "Step 50, train loss = 0.67, train accuracy = 60.01%\n",
      "Step 100, train loss = 0.64, train accuracy = 66.02%\n",
      "Step 150, train loss = 0.62, train accuracy = 66.02%\n",
      "Step 200, train loss = 0.60, train accuracy = 71.48%\n",
      "Step 250, train loss = 0.58, train accuracy = 70.51%\n",
      "Step 300, train loss = 0.52, train accuracy = 74.51%\n",
      "Step 350, train loss = 0.53, train accuracy = 75.00%\n",
      "Step 400, train loss = 0.58, train accuracy = 71.48%\n",
      "Step 450, train loss = 0.40, train accuracy = 85.50%\n",
      "Step 500, train loss = 0.41, train accuracy = 82.52%\n",
      "Step 550, train loss = 0.41, train accuracy = 79.49%\n",
      "Step 600, train loss = 0.44, train accuracy = 79.98%\n",
      "Step 650, train loss = 0.34, train accuracy = 85.99%\n",
      "Step 700, train loss = 0.32, train accuracy = 85.99%\n",
      "Step 750, train loss = 0.26, train accuracy = 91.99%\n",
      "Step 800, train loss = 0.28, train accuracy = 86.52%\n",
      "Step 850, train loss = 0.18, train accuracy = 93.99%\n",
      "Step 900, train loss = 0.16, train accuracy = 93.99%\n",
      "Step 950, train loss = 0.13, train accuracy = 95.02%\n",
      "test loss = 0.69, test accuracy = 58.50%\n"
     ]
    }
   ],
   "source": [
    "#======================================================================  \n",
    "#导入文件  \n",
    "import os  \n",
    "import numpy as np  \n",
    "import tensorflow as tf  \n",
    "#import input_data  \n",
    "#import model  \n",
    "  \n",
    "#变量声明  \n",
    "N_CLASSES = 2  #bhg,hg  \n",
    "IMG_W = 64   # resize图像，太大的话训练时间久  \n",
    "IMG_H = 64  \n",
    "BATCH_SIZE =200  \n",
    "CAPACITY = 400  \n",
    "MAX_STEP = 1000 # 一般大于10K  \n",
    "learning_rate = 0.0001 # 一般小于0.0001  \n",
    "  \n",
    "#获取批次batch  \n",
    "train_dir = 'C:/Users/sungroup/PycharmProjects/anti_cnn/pavement'   #训练样本的读入路径  \n",
    "logs_train_dir = 'C:/Users/sungroup/PycharmProjects/anti_cnn'    #logs存储路径  \n",
    "#logs_test_dir =  'E:/Re_train/image_data/test'        #logs存储路径  \n",
    "  \n",
    "#train, train_label = input_data.get_files(train_dir)  \n",
    "train, train_label, val, val_label = get_files(train_dir, 0.1)  \n",
    "#训练数据及标签  \n",
    "train_batch,train_label_batch = get_batch(train, train_label, IMG_W, IMG_H, BATCH_SIZE, CAPACITY)  \n",
    "#测试数据及标签  \n",
    "val_batch, val_label_batch = get_batch(val, val_label, IMG_W, IMG_H, BATCH_SIZE, CAPACITY)   \n",
    "  \n",
    "#训练操作定义  \n",
    "train_logits = inference(train_batch, BATCH_SIZE, N_CLASSES)  \n",
    "train_loss = losses(train_logits, train_label_batch)          \n",
    "train_op = trainning(train_loss, learning_rate)  \n",
    "train_acc = evaluation(train_logits, train_label_batch)  \n",
    "  \n",
    "#测试操作定义  \n",
    "test_logits = inference(val_batch, BATCH_SIZE, N_CLASSES)  \n",
    "test_loss = losses(test_logits, val_label_batch)          \n",
    "test_acc = evaluation(test_logits, val_label_batch)  \n",
    "test_cm = print_confusion_matrix(test_logits, val_label_batch)\n",
    "#这个是log汇总记录  \n",
    "summary_op = tf.summary.merge_all()   \n",
    "  \n",
    "#产生一个会话  \n",
    "sess = tf.Session()    \n",
    "#产生一个writer来写log文件  \n",
    "train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph)   \n",
    "#val_writer = tf.summary.FileWriter(logs_test_dir, sess.graph)   \n",
    "#产生一个saver来存储训练好的模型  \n",
    "saver = tf.train.Saver()  \n",
    "#所有节点初始化  \n",
    "sess.run(tf.global_variables_initializer())    \n",
    "#队列监控  \n",
    "coord = tf.train.Coordinator()  \n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)  \n",
    "  \n",
    "#进行batch的训练  \n",
    "try:  \n",
    "    #执行MAX_STEP步的训练，一步一个batch  \n",
    "    for step in np.arange(MAX_STEP):  \n",
    "        if coord.should_stop():  \n",
    "            break  \n",
    "        #启动以下操作节点，有个疑问，为什么train_logits在这里没有开启？  \n",
    "        _, tra_loss, tra_acc = sess.run([train_op, train_loss, train_acc])  \n",
    "          \n",
    "        #每隔50步打印一次当前的loss以及acc，同时记录log，写入writer     \n",
    "        if step % 50  == 0:\n",
    "            print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))\n",
    "            summary_str = sess.run(summary_op)  \n",
    "            train_writer.add_summary(summary_str, step)  \n",
    "        #每隔100步，保存一次训练好的模型  \n",
    "        if (step + 1) == MAX_STEP:\n",
    "            val_loss, val_acc = sess.run([test_loss, test_acc])\n",
    "            print('test loss = %.2f, test accuracy = %.2f%%' %(val_loss, val_acc*100.0))\n",
    "            val_cm= sess.run(test_cm)\n",
    "            print(val_cm)\n",
    "            checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')  \n",
    "            saver.save(sess, checkpoint_path, global_step=step)  \n",
    "         \n",
    "except tf.errors.OutOfRangeError:  \n",
    "    print('Done training -- epoch limit reached')  \n",
    "  \n",
    "finally:  \n",
    "    coord.request_stop()  \n",
    "      \n",
    "#========================================================================</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batch_1:0\", shape=(20, 400, 400, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix():\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = np.array(val_label_batch)\n",
    " \n",
    "    # Get the predicted classifications for the test-set.\n",
    "    prediction = sess.run(predicted, feed_dict=feed_dict_test)\n",
    "    y_pred_cls = tf.argmax(prediction, dimension=1)\n",
    "    cls_pred = sess.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "\n",
    " \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    " \n",
    "    # Print the confusion matrix as text.\n",
    "\n",
    "    print(cm)\n",
    " \n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    " \n",
    "    # Make various adjustments to the plot.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(2),('unqualified','qualified'))\n",
    "    plt.yticks(np.arange(2),('unqualified','qualified'))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    #prediction = sess.run(predicted, feed_dict=feed).squeeze\n",
    "    #prediction = sess.run(predicted, feed_dict=feed)\n",
    "    #y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "    #print(prediction.shape)\n",
    "    print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
